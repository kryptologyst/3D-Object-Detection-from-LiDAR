# @package training

# Training configuration
epochs: 100
learning_rate: 0.001
weight_decay: 0.0001

# Optimizer
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.001
  weight_decay: 0.0001
  betas: [0.9, 0.999]

# Learning rate scheduler
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 100
  eta_min: 0.0001

# Gradient clipping
gradient_clip_val: 1.0

# Validation
val_check_interval: 0.5  # Validate every 0.5 epochs
save_top_k: 3
monitor: "val/mAP_3D"
mode: "max"

# Early stopping
early_stopping:
  patience: 10
  monitor: "val/mAP_3D"
  mode: "max"
